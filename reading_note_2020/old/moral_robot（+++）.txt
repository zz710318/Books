道德机器：如何让机器人明辨是非
#AMAS：人工道德智能体
1、一个好的道德智能体能够侦测肯呢个的伤害或者对责任的疏忽，并能够采取措施避免或降低哪些不想要的结果。而这可以通过两条路径来实现：
    1）程序员也许能够预估各种可能事态并提供规则导向在AMAS使用环境范围内的期望结果
    2）程序员可以建立一个更加开放的系统，收集信息，试图去预测其行动后果。并会自定义响应，去应对挑战。这样的系统甚至有潜能，以其明显创新或创造性解决伦理挑战的方式令它的程序员感到惊讶
2、机器越自有就越需要道德准则
3、自主性和对价值的敏感性
4、越来越成熟的功能性道德形式，到最终完全成熟的AMAS，实际上将使公司在经济上获益，这种获益的来源之一，就是这些系统将可能使得公司提供比其竞争者更好的服务
5、按理说，所有的机器人都应该被设计成隐含式伦理智能体，如果没有在设计过程在建构安全性和可靠性保障，那就是设计者疏忽大意了。（隐式伦理智能体）
6、通过将伦理范畴作为内置程序进行伦理问题的推理，或许通过各种各样的用来代表责任和义务的“道义逻辑”或其他技术来实现（显式伦理智能体）
7、道德决策的机械化，这个尝试对人类带来什么后果？这项努力将机器变成智能自主体。会像炼金术师門把铅变成金一样误入歧途么？
8、害怕技术脱离人类控制
9、某种程度上对人来进行定义的工具，也可以视为在控制人的人命和对人的自主性进行侵蚀
10、在我们现金对未来技术的许多思虑背后，隐藏着这样一个问题。这个问题不是未来技术会是什么样子。而是我们会是什么样子。随着我们与机器建立日益亲密的关系，我们将变成什么。
11、”作为一种人性化力量的人工智能“，“人工智能可以是‘西方人的芒果书’，有能力把人类从单调乏味的工作中解放出来，去追求更加人文化的活动”
12、计算机系统的模块化设计。意味着没有一个单独的个体或团队可以掌握系统与一个复杂新输入流进行互动或回应的方法。人工道德的目标，将使工程能动主义不仅仅只是关注设计者在塑造系统的操作性道德价值方面的角色，而是要进入到为系统自身提供清晰的道德推理和决策能力。
13、最有可能的机器道德初始进路是采取针对决策者的软件支持工具的形式。但是有一个危险，使用者把这种支持工具当拐杖，用机器的输出代替自己的批判思考。
14、DSTS（决策支持工具）DSTS会发起一个导向人类决策者放弃道德责任的滑坡谬误。随着人们相信了DSTS的建议，质疑他的建议就变得更难。于是他们认为存在一种危险，DSTS最终可能控制决策过程。
15、森政弘：人类对于具有人类特征和行为的机器人会感到更舒适并移情。而一旦机器人开始和人类过于相像。人类就会变得非常不舒服，甚至感到厌恶。看起来人像人类，却没有满足人们期望所造成的不协调显然会令人非常不安。莫里（mori）将这种舒适感的陡降描绘为“恐怖谷”--假定如果类人机器人能被设计的更像人，克服这种不适感将是可能的。
16、大多数人都认为机器人娃娃是很差的人类陪伴替代品。而且认为，疗养院房客对其产生的这种依恋，表明社会在满足老年人和行动不便者的情感需求方面是失败的。人们也许痛恨任何采取社交机器人来解决人们孤单和交往需求的建议。
但是，关于响应人们需求的社交机器人方面需要回答一些难题。涉及它们的作用，实效性以及适当性。例如，如果没有证据表明人们和社区愿意安排时间或资源去满足老年人和残疾人与人接触的需要，那么机器人是不是聊剩与无呢？
17、生活中接受机器人会弱化人们珍视的价值并贬低人性么？ 罗纳德.阿金 佐治亚理工大学
18、评估新技术的影响还远远谈不上是科学，就药物安全性，土建工程和复杂技术所做的风险评估报告。充满了涉及许许多多因素的数据。最后呢，某个人必须去说明每个因素的相关输入，而且可量化的研究让位给价值评判，经验数据常常是用作掩饰，而事实上，某个集团的经济或政治利益才是对最终的评估起着重大的影响。
19、仔细检查一份风险评估的形式过程，其价值在于针对可预见的风险来衡量可预见的利好。
20、物质、生命和心灵之谜定义了科学的三大挑战
21、所有人都知道，物理中需要的是更强大的粒子加速器和天文望远镜，而生物中需要的是能对基因和细胞进行分子水平操作的更好的技术
22、计算机程序是可以用来控制物理机器操作的一系列形式化的符号  “物理符号系统” --卡耐基梅隆
23、人工施事者的概念中三个重要特征
    1）互动性：与环境互相影响
    2）自主性：无刺激情况下改变状态的能力
    3）适应性：使能够致使状态改变的“转换规则”进行改变的能力
24、人类道德经验的一个核心特征，就是人们经常觉得自己会在自私的行为和利他的行为之间徘徊。一些伦理学家甚至认为。如果没有不道德的行动，道德的行动也是不可能的。
25、传统观点认为，大脑必须建造外部世界的完整的内部表征 -- 一个包含推理如何行动时所需的全部细节的完整的模型或者模拟。而具身认知理论作为另一种选择的可能出现了，在传统的更加中央化的认知理论中，大脑对构成世界模型的内部符号做出操作来确定每个行动，每个反应，例如每个肌肉或关节的位置，但是，这样设计出的机器人系统会很脆弱。
26、道德是进化的，人工道德智能体在活动的很多领域中成为迎接新挑战的积极参与者
27、“理解”意味着什么？如果它意味着适当，适应性的对社会环境和物理环境做出反应的能力，我们认为，没有任何理由能够证明合适的具身。嵌入式计算机不能有这些反应。已经有工程师在研发“生成式”的人机接口了 -- 使用者通过全感官的方式参与到其中来，而不是只把互动限制在语音山上。随着这样的系统不断增加智能，所有的理解是否仅仅存在与系统的数字编程部分的问题也会越来越无关紧要。
28、就像最初的图灵测试一样，任何基于机器的行为与人类的行为相比较的道德图灵测试。注定远非一个完美的评价工具。但是，思考这个测试的局限能够帮助我们确定，对评价人工道德智能体来说什么可能是重要的。
29、人类并不是纯善的
30、经过规划考虑后仍对他人造成伤害的决定如果发生在机器上，会比发生在人类身上更不能令人容忍。换句话说：我们可能期待机器比我们自己做的更好
31、军事机器人可能是最先需要人工智能道德的地方。
32、期望人工智能道德体能立即解决所有的问题是不切实际的，但我们的基本立场是，任何能够提高机器人道德考量的敏感性的进步，无论多微小，都是朝着正确的方向迈进。
33、宣称能将伦理学还原到科学的主张起码是幼稚的
34、制造一个有道德的机器人就变成区发现一系列正确的限制条件和解决冲突的合适方案。开发人工智能体的难题因此被理解为在智能系统的控制结构内寻找执行抽象价值理念的方法。结果就会成为一种“约束性道德”，并且只要系统遇到的情况能适用设计者预测的一般性限制条件，它都能以不侵犯他人的方式行动
35、“谁的道德或什么样的道德” “谁有权利决定什么是道德？”
36、执行道德规则向形式决策算法转换的前景是多么黯淡
37、规则列表，只是对于需要明确规定或禁止的所有行为的一种任意集合。这就是道德的戒律模式
38、戒律模式存在的挑战是，当规则相互冲突了怎么办，存在为了解决冲突而设定的更深层次的原则或规则么？ ==> 自上而下的规则可以起到启发式的作用
39、道义论：知道规则，并且有办法在一个具体的挑战中运用这些规则是最为关键的，如果一个智能体能够对具有规则的有效性做出连贯的反应，那将是令人满意的，但这是一个遥远的梦想  道义论机器人的设计者需要想办法确保，当环境需要应用规则时这些规则能被激活。还要制定一个架构来设法解决规则冲突的情况
40、天气预报员使用一个特别有效的技术，是对几个对等的计算模型的预测进行平均
41、只主义某些偏好，对这些偏好产生错误的评估，或未能理解行为的短期和长期的后果。终将招致痛楚与苦难。
42、绝对率令  康德
43、人类并非生来就是合格的道德智能体，当然也不是所有人离世时都能达到这一水平
44、任何说人类是由于“基因程控”而变得道德的人对基因的运作方式都想得太简单
45、先天性+后天性习得
46、重复的囚徒困境博弈已经成为研究合作现象出现的基础
47、《自私的基因》 对基因有利的未必对个体有利
48、进化过程中产生的倾向、能力、特征，不仅仅是个体为了生存和繁衍而努力的产物，它们还是在多物种构成的环境中进行社会互动和成功适应的结果
49、乔木纳基的人类语言习得法就示例了一条重要进路，即将学习视为从预先确定的可能性中找到表征事实的最好方式的问题，另一种进路是把学习者更多的看成是一块白板，不仅要面对找出对事实的合适表达的任务，同时也要面对着找出一种合适的表述机制的任务，传统意义上，人工智能的符号进路趋向于把学习当成是对一套预先确定的概念重新组合起来的过程。最近，更多的联结主义进路不太依赖于事先形成的结构，而是利用人工神经网路的能力。从所接手的输入中，动态的形成自己的分类表
50、任何为规则所约束的系统，将接受其行为中注定的僵化刻板折磨
51、进化和学习是非常缓慢的过程。即使在计算机中可以在几秒内实现很多代人工智能体的突变和复制/
52、不要把所有的鸡蛋丢进同一个篮子
53、道德源自多只能替的互动，它们必须平衡自己的需求和他人的竞争性需求
54、“第二人生” 虚拟环境 270 => 1
55、道德软件正处于起步阶段
56、伦理学家长期以来都承认羞耻感觉，负罪感以及其他情绪在规范人类行为方面起着核心作用。但他们更关心这些情绪否应该起这种作用
57、没有人研究嗅觉在社交型机器人中可能发挥的作用
58、一个完整的系统需要留心这样一件事实，即在人类中，疼痛通常是与具体情况相关的，并且依赖于一系列因素的整合
59、拥有可以识别情绪的计算机和机器人： 模拟人类阅读非语言线索（面部表情、语调、身体姿势、...）的能力，正是这些非语言线索帮助了人们。互相理解彼此
60、OCC模型、Ortony、Clore、Cdins：关于情绪的计算认知模型
61、如果人们渐渐觉得人工道德智能体的行为是值得信赖的，那么它们的行动的接手度和范围的相应变化也将成为变化的社交语境的一部分，相反，如果人工道德智能体不能采取合适的行动，那么他们将不得不适应哪些将对他们的行为进行的附加限制
62、托兰斯指出，道德观点可以说需要对处在痛苦或悲伤中的人感到同情，而不仅仅是在理性推断出帮助他人是正确的事的基础上付诸行动，没有实际感受悲伤、痛苦、恐惧、愤怒或积极情感，如喜悦、快乐、感激和爱慕的能力，一个人便不具备成为完整、全面的道德个体所必须的道德状态或道德认同感。在这种伦理观中，如果不考虑人的感性，生物构造和历史。就不能正确的理解拥有这些状态的能力。换句话说，同理心，同情心，感性和道德是捆绑在一起的。
63、试图对人类智能进行全盘复制的系统通常被称为拥有通用人工智能或者强人工智能
64、工程师的信条是：除非你把它造出来了，够则你就不会真正明白它是运作的
65、Ray.kurzwell, Hans.moravec等著名科学家激情洋溢的系列，机器人的智商将高出与人类，人类可以把精神上传到计算机系统而获得某种形式的永生？
66、一个机器人给另外一个机器人重新编程的可能性
67、系统越来越复杂，出现问题时，问责就变得极为空难
68、在不久的将来，会继续沿用产品安全法去处理人工智能体问题，对非法的，不负责任的危险实践而言，实际的追则将首先由法庭确立，随后才是立法机构。智能机器人对现有的法律提出很多的新的挑战。
69、我们在本书中一直在论证，人工系统是否是真正的道德主体，这并不重要
70、工程学的目标始终如一：人类需要高级机器人尽可能像道德主题一样行使
71、所以，为了眼前紧迫的实践目的，我们要设计和分配伤害责任（软件工程和社会工程方面的），我们认为，不要过多的纠结在机器人和软件主体是否真的是道德自主体。不过，看一看关于真正的道德主体性方面的哲学讨论，了解这些论证是否提供了线索，以预期并解决自主机器人将带来的法律和政治问题，仍然是具有指导意义的
72、艾萨克.阿西莫夫 《The Bicontennial Man》得到一个结论：人类的道德源于死亡

73、所有新技术都面临诺维格的问题 /诺维格：Google 无人驾驶
74、在本书中，我们已经努力绘制了自主机器人可能成为人工道德智能体的路径（尽管不乏人声称自己知道！）但无人知晓道德机器的最终技术极限何在，或究竟有没有极限。工程师和伦理学家必须认识到现有技术的极限，并利用最好的技术所提供的伦理能力。对技术能力过于乐观的估计，可能导致人们去依赖哪些对伦理考量不够命案的机器人，这是岌岌可危的，而过于悲观的估计则可能妨碍某些真正有用技术的发展，或引起人们对机器人产生一种宿命论的态度
75、我们相信，继续推进人工道德智能体的发展势在必行，至于未来的机器人是不是“真的”道德智能体，这无关紧要，我们有可能设计，建造出能够考量法律和道德因素而去实施决策的系统，而且会比现有的系统更为命案，未来需要AMAS。

所有做机器人的都该读一下~
“谁的道德或什么样的道德” “谁有权利决定什么是道德？”
